<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yang&#39;s Den</title>
    <link>https://songyang0716.github.io/index.xml</link>
    <description>Recent content on Yang&#39;s Den</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Sep 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://songyang0716.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Scrape daily stock price and deliver to your email with python</title>
      <link>https://songyang0716.github.io/2017/09/scrape-daily-stock-price-and-deliver-to-your-email-with-python/</link>
      <pubDate>Fri, 29 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://songyang0716.github.io/2017/09/scrape-daily-stock-price-and-deliver-to-your-email-with-python/</guid>
      <description>&lt;p&gt;A tutorial on website scraping &amp;amp; email delivering using python
&lt;/p&gt;

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;As I think back, I&amp;rsquo;ve always been involved with the investment market unintentionally. As a kid, my parents always talk with their financial advisors about their mutual funds, look for new investment opportunities in the market. They urged me to buy some bonds with my pocket money during college. However, at that time, I didn&amp;rsquo;t really know what money really was and meant. After I graduated from school around two years ago, I started thinking about investment. I have a little extra money that I can roll the dice with. I already contribute to my 401k and also have the majority of my money in the mutual funds, which apparently are less risky than a stock. However, I want to try some stocks to help me understand money and market better. I picked a couple of stocks and purchased a few shares in each of them. However, I am tired of checking the stock information using my phone. So I am thinking about to retrieve the stock indices automatically from the internet and then send to my personal email. One could also set up a automater to run the script every hour.&lt;/p&gt;

&lt;h3 id=&#34;code-review&#34;&gt;Code Review&lt;/h3&gt;

&lt;p&gt;The coding involves two methods,the first method is called scrape, which uses python &lt;a href=&#34;http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html&#34;&gt;BeautifulSoup&lt;/a&gt; package to extract the stock indices for specific stocks that we care about. The second method is called email, which will send the information to the specific email addresses using the python &lt;a href=&#34;https://docs.python.org/3/library/smtplib.html&#34;&gt;smtplib&lt;/a&gt; package.&lt;/p&gt;

&lt;h4 id=&#34;python-code&#34;&gt;Python Code&lt;/h4&gt;


  
    
  
  
    
  
  
    
  
  
    
  


&lt;figure class=&#34;highlight python&#34;&gt;
  &lt;figcaption&gt;
    
      &lt;span&gt;scraper.py&lt;/span&gt;&lt;a href=&#34;http://underscorejs.org/#compact&#34; target=&#34;_blank&#34; rel=&#34;external&#34;&gt;scraper.py&lt;/a&gt;
    
  &lt;/figcaption&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td class=&#34;gutter&#34;&gt;
          &lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;83&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;84&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;85&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;86&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;87&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;88&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;89&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;90&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;
        &lt;/td&gt;
        &lt;td class=&#34;code&#34;&gt;
          &lt;pre class=&#34;python code-highlight&#34;&gt;import urllib2
from bs4 import BeautifulSoup
from time import sleep
import smtplib
import getpass
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from collections import defaultdict

class stock_scraper:
    def __init__(self, stocks = [&amp;#39;SPX:IND&amp;#39;,&amp;#39;INDU:IND&amp;#39;,&amp;#39;CCMP:IND&amp;#39;,&amp;#39;SNAP:US&amp;#39;,&amp;#39;DLPH:US&amp;#39;, &amp;#39;HD:US&amp;#39;], sender_email = &amp;#39;sender email&amp;#39;, receiver_email = [&amp;#39;receiver email&amp;#39;]) :
        &amp;#34;&amp;#34;&amp;#34;
        Stocks : Default contains five stocks
        All using stocks ticker symbol

        sender_email : From which email address do you want to send the file
        receiver_email : Which email address to send the file
        &amp;#34;&amp;#34;&amp;#34;
        self.stock_performance = defaultdict(list)
        self.scrape(stocks)
        self.email(sender_email, receiver_email)

    def scrape(self, stocks) :
        &amp;#34;&amp;#34;&amp;#34;
        stocks : list of stocks ticker symbol
        &amp;#34;&amp;#34;&amp;#34;
        for stock in stocks:
            quote_page = &amp;#39;http://www.bloomberg.com/quote/&amp;#39; &amp;#43; stock
            page = urllib2.urlopen(quote_page)
            soup = BeautifulSoup(page, &amp;#39;html.parser&amp;#39;)

            name_box = soup.find(&amp;#39;h1&amp;#39;, attrs={&amp;#39;class&amp;#39;: &amp;#39;name&amp;#39;})
            name = name_box.text
            price_box = soup.find(&amp;#39;div&amp;#39;, attrs = {&amp;#39;class&amp;#39;:&amp;#39;price&amp;#39;})
            price = price_box.text
            change_box = soup.find(&amp;#39;div&amp;#39;, attrs = {&amp;#39;class&amp;#39;:&amp;#39;change-container&amp;#39;})
            percent_change = change_box.text.strip().split(&amp;#39;   &amp;#39;)[1]
            percent_change = float(percent_change.split(&amp;#34;%&amp;#34;)[0])

            if not soup.find(&amp;#39;div&amp;#39;, attrs = {&amp;#39;class&amp;#39;:&amp;#39;price-container up&amp;#39;}):
                percent_change *= -1

            if percent_change and price and name:
                self.stock_performance[name].append(price)
                self.stock_performance[name].append(percent_change)

            sleep(2)

        return

    def email(self, sender, receivers) :
        &amp;#34;&amp;#34;&amp;#34;
        from : sender&amp;#39;s email address
        to: receivers&amp;#39; email addresses
        &amp;#34;&amp;#34;&amp;#34;
        msg = MIMEMultipart()
        msg[&amp;#39;Subject&amp;#39;] = &amp;#39;Daily Stock Performance&amp;#39;
        msg[&amp;#39;From&amp;#39;] = sender
        msg[&amp;#39;To&amp;#39;] = &amp;#34;,&amp;#34;.join(receivers)
        msg.preamble = &amp;#39;Daily Stock Performance&amp;#39;

        message = &amp;#34;Hello!\n&amp;#34;
        message &amp;#43;= &amp;#34;\n&amp;#34;
        for stock_name, value in self.stock_performance.iteritems():
            if value[1] &amp;gt;= 0 :
                message &amp;#43;= (stock_name &amp;#43; &amp;#39;is doing good today! &amp;#39; &amp;#43; &amp;#39;The stock price increases &amp;#39; &amp;#43; str(value[1]) &amp;#43; &amp;#39;%, The current stock price is $&amp;#39; &amp;#43; str(value[0])) &amp;#43; &amp;#39;\n&amp;#39;
                message &amp;#43;= &amp;#34;\n&amp;#34;
            else:
                message &amp;#43;= (stock_name &amp;#43; &amp;#39;is not doing good today! &amp;#39; &amp;#43; &amp;#39;The stock price decreases &amp;#39; &amp;#43; str(value[1]) &amp;#43; &amp;#39;%, The current stock price is $&amp;#39; &amp;#43; str(value[0])) &amp;#43; &amp;#39;\n&amp;#39;
                message &amp;#43;= &amp;#34;\n&amp;#34;


        msg.attach(MIMEText(message, &amp;#39;plain&amp;#39;))

        pw = &amp;#39;password&amp;#39;
        s = smtplib.SMTP(host=&amp;#39;smtp.gmail.com&amp;#39;, port=587)
        s.starttls()
        try:
            s.login(sender, pw)
            print &amp;#34;Your email is sent, thank you !&amp;#34;
        except SMTPAuthenticationError:
            print &amp;#34;Wrong password dude !&amp;#34;
            return

        s.sendmail(sender, receivers, msg.as_string())

        del msg
        s.quit()

        return&lt;/pre&gt;
        &lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>An implementation of noisy channel model</title>
      <link>https://songyang0716.github.io/2017/06/an-implementation-of-noisy-channel-model/</link>
      <pubDate>Mon, 19 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://songyang0716.github.io/2017/06/an-implementation-of-noisy-channel-model/</guid>
      <description>&lt;p&gt;A simple implementation of noisy channel model to correct the non-word, real-word spelling errors occured in a sentence
&lt;/p&gt;

&lt;h3 id=&#34;method&#34;&gt;Method&lt;/h3&gt;

&lt;p&gt;I learned this topic from the 5th chapter of the book &lt;strong&gt;Speech and Language Processing&lt;/strong&gt; by &lt;strong&gt;Daniel Jurafsky &amp;amp; James H. Martin&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The algorithm takes the input sentence X = {x1, x2, x3, x4&amp;hellip; xn}, which might contains some spelling errors, next generates a large set of candidate correction sentences C(X), finally picks the sentence with the highest language model probability.&lt;/p&gt;

&lt;p&gt;To generate the candidate correction sentences, I start by generating a set of candidate words for each input word xi. I used the python package &lt;strong&gt;PyEnchant&lt;/strong&gt; to find these candidates word. Then each new proposed sentences is scored by the noisy channel In formula, W_hat = argmax(P(X/W) * P(W)), X is the sentence we observed, W represents one of our candidate sentences. For P(W), I use the &lt;strong&gt;bigram stupid backoff with laplace smoothing&lt;/strong&gt;. For the channel model, P(X/W) will be 0.95 when x = w, and then distribute the rest 0.05 evenly over all other candidate corrections C(X).&lt;/p&gt;

&lt;p&gt;The training dataset I used is from &lt;strong&gt;Peter Norvig&lt;/strong&gt;. You could have access to this file in &lt;a href=&#34;http://norvig.com/big.txt&#34;&gt;http://norvig.com/big.txt&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;python-code&#34;&gt;Python Code&lt;/h4&gt;


  
    
  
  
    
  
  
    
  
  
    
  


&lt;figure class=&#34;highlight python&#34;&gt;
  &lt;figcaption&gt;
    
      &lt;span&gt;spell.py&lt;/span&gt;&lt;a href=&#34;http://underscorejs.org/#compact&#34; target=&#34;_blank&#34; rel=&#34;external&#34;&gt;spell.py&lt;/a&gt;
    
  &lt;/figcaption&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td class=&#34;gutter&#34;&gt;
          &lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;83&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;84&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;85&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;86&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;87&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;88&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;89&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;90&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;91&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;92&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;93&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;94&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;95&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;96&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;97&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;98&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;99&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;100&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;101&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;102&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;103&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;104&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;105&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;106&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;107&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;108&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;109&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;110&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;111&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;112&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;113&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;114&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;115&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;116&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;117&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;118&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;119&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;120&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;121&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;122&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;123&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;124&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;125&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;126&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;127&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;128&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;129&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;130&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;131&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;132&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;133&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;134&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;
        &lt;/td&gt;
        &lt;td class=&#34;code&#34;&gt;
          &lt;pre class=&#34;python code-highlight&#34;&gt;import enchant
import numpy as np
import csv
import math, collections
import pandas as pd
import re
import itertools
from nltk import tokenize
import nltk.data


class Sentence_Corrector :
    def __init__(self, training_file) :
        self.laplaceUnigramCounts = collections.defaultdict(lambda: 0)
        self.laplaceBigramCounts = collections.defaultdict(lambda: 0)
        self.total = 0
        self.sentences = []
        self.importantKeywords = set()
        self.d = enchant.Dict(&amp;#34;en_US&amp;#34;)
        self.tokenize_file(training_file)
        self.train()

    def tokenize_file(self, file) :
        # &amp;#34;&amp;#34;&amp;#34;
        #   Read the file, tokenize and build a list of sentences
        # &amp;#34;&amp;#34;&amp;#34;
        tokenizer = nltk.data.load(&amp;#39;tokenizers/punkt/english.pickle&amp;#39;)
        f = open(file)
        content = f.read()
        for sentence in tokenizer.tokenize(content):
            sentence_clean = [i.lower() for i in re.split(&amp;#39;[^a-zA-Z]&amp;#43;&amp;#39;, sentence) if i]
            self.sentences.append(sentence_clean)


    def train(self):
        # &amp;#34;&amp;#34;&amp;#34;
        #   Train unigram and bigram
        # &amp;#34;&amp;#34;&amp;#34;
        for sentence in self.sentences:
            sentence.insert(0, &amp;#39;&amp;lt;s&amp;gt;&amp;#39;)
            sentence.append(&amp;#39;&amp;lt;/s&amp;gt;&amp;#39;)
            for i in xrange(len(sentence) - 1):
                token1 = sentence[i]
                token2 = sentence[i &amp;#43; 1]
                self.laplaceUnigramCounts[token1] &amp;#43;= 1
                self.laplaceBigramCounts[(token1, token2)] &amp;#43;= 1
                self.total &amp;#43;= 1
            self.total &amp;#43;= 1
            self.laplaceUnigramCounts[sentence[-1]] &amp;#43;= 1


    def candidate_word(self, word):
        # &amp;#34;&amp;#34;&amp;#34;
        # Generate similar word for a given word
        # &amp;#34;&amp;#34;&amp;#34;
        suggests = []
        for candidate in self.importantKeywords:
            if candidate.startswith(word):
                suggests.append(candidate)
        suggests.append(word)

        if len(suggests) == 1:
            suggests = self.d.suggest(word)
            suggests = [suggest.lower() for suggest in suggests][:4]
            suggests.append(word)
            suggests = list(set(suggests))

        return suggests, len(suggests)

    def candidate_sentence(self, sentence):
        # &amp;#34;&amp;#34;&amp;#34;
        # Takes one sentence, and return all the possible sentences, and also return a dictionary of word : suggested number of words
        # &amp;#34;&amp;#34;&amp;#34;
        candidate_sentences = []
        words_count = {}
        for word in sentence:
            candidate_sentences.append(self.candidate_word(word)[0])
            words_count[word] = self.candidate_word(word)[1]

        candidate_sentences = list(itertools.product(*candidate_sentences))
        return candidate_sentences, words_count

    def correction_score(self, words_count, old_sentence, new_sentence) :
        # &amp;#34;&amp;#34;&amp;#34;
        #   Take a old sentence and a new sentence, for each words in the new sentence, if it&amp;#39;s same as the orginal sentence, assign 0.95 prob
        #   If it&amp;#39;s not same as original sentence, give 0.05 / (count(similarword) - 1)
        # &amp;#34;&amp;#34;&amp;#34;
        score = 1
        for i in xrange(len(new_sentence)) :
            if new_sentence[i] in words_count :
                score *= 0.95
            else :
                score *= (0.05 / (words_count[old_sentence[i]] - 1))
        return math.log(score)

    def score(self, sentence):
        # &amp;#34;&amp;#34;&amp;#34;
        #     Takes a list of strings as argument and returns the log-probability of the
        #     sentence using the stupid backoff language model.
        #     Use laplace smoothing to avoid new words with 0 probability
        # &amp;#34;&amp;#34;&amp;#34;
        score = 0.0
        for i in range(len(sentence) - 1):
            if self.laplaceBigramCounts[(sentence[i],sentence[i &amp;#43; 1])] &amp;gt; 0:
                score &amp;#43;= math.log(self.laplaceBigramCounts[(sentence[i],sentence[i &amp;#43; 1])])
                score -= math.log(self.laplaceUnigramCounts[sentence[i]])
            else:
                score &amp;#43;= (math.log(self.laplaceUnigramCounts[sentence[i &amp;#43; 1]] &amp;#43; 1) &amp;#43; math.log(0.4))
                score -= math.log(self.total &amp;#43; len(self.laplaceUnigramCounts))
        return score

    def return_best_sentence(self, old_sentence) :
        # &amp;#34;&amp;#34;&amp;#34;
        #   Generate all candiate sentences and
        #   Calculate the prob of each one and return the one with highest probability
        #   Probability involves two part 1. correct probability and 2. language model prob
        #   correct prob : p(c | w)
        #   language model prob : use stupid backoff algorithm
        # &amp;#34;&amp;#34;&amp;#34;
        bestScore = float(&amp;#39;-inf&amp;#39;)
        bestSentence = []
        old_sentence = [word.lower() for word in old_sentence.split()]
        sentences, word_count = self.candidate_sentence(old_sentence)
        for new_sentence in sentences:
            new_sentence = list(new_sentence)
            score = self.correction_score(word_count, new_sentence, old_sentence)
            new_sentence.insert(0, &amp;#39;&amp;lt;s&amp;gt;&amp;#39;)
            new_sentence.append(&amp;#39;&amp;lt;/s&amp;gt;&amp;#39;)
            score &amp;#43;= self.score(new_sentence)
            if score &amp;gt;= bestScore:
                bestScore = score
                bestSentence = new_sentence
        bestSentence = &amp;#39; &amp;#39;.join(bestSentence[1:-1])
        return bestSentence, bestScore&lt;/pre&gt;
        &lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/figure&gt;

&lt;h4 id=&#34;usage-examples&#34;&gt;Usage Examples&lt;/h4&gt;

&lt;p&gt;
  
    
  
  
    
  
  
    
  
  
    
  


&lt;figure class=&#34;highlight python&#34;&gt;
  &lt;figcaption&gt;
    
      &lt;span&gt;spell.py&lt;/span&gt;&lt;a href=&#34;http://underscorejs.org/#compact&#34; target=&#34;_blank&#34; rel=&#34;external&#34;&gt;spell.py&lt;/a&gt;
    
  &lt;/figcaption&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td class=&#34;gutter&#34;&gt;
          &lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;
        &lt;/td&gt;
        &lt;td class=&#34;code&#34;&gt;
          &lt;pre class=&#34;python code-highlight&#34;&gt;corrector = Sentence_Corrector(&amp;#39;../data/big.txt&amp;#39;)
corrector.return_best_sentence(&amp;#39;this is wron spallin word&amp;#39;)
corrector.return_best_sentence(&amp;#39;aoccdrning to a resarch at cmabridge university&amp;#39;)
corrector.return_best_sentence(&amp;#39;it does not mttaer in waht oredr the ltteers&amp;#39;)
corrector.return_best_sentence(&amp;#39;the olny important tihng is taht&amp;#39;)
corrector.return_best_sentence(&amp;#39;Can they leav him my messages&amp;#39;)
corrector.return_best_sentence(&amp;#39;This used to belong to thew queen&amp;#39;)&lt;/pre&gt;
        &lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/figure&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Optimal Stopping in Speed Dating</title>
      <link>https://songyang0716.github.io/2017/05/optimal-stopping-in-speed-dating/</link>
      <pubDate>Tue, 02 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://songyang0716.github.io/2017/05/optimal-stopping-in-speed-dating/</guid>
      <description>&lt;p&gt;I came across this question when I was reading the first chapter of the book ‘Algorithms to Live By’. It’s a famous problem that uses the optimal stopping theory. The problem has been studied extensively in the fields of statistics, decision theory and applied probability. It’s also known as secretary problem, marriage problem and the best choice problem.In any optimal problem, the crucial dilemma is not which option to &lt;em&gt;pick&lt;/em&gt;, but how many options to even &lt;em&gt;consider&lt;/em&gt;. The problem proved to be a near-perfect mathematical puzzle: simple to explain, devilish to solve, succinct in its answer, and intriguing in its implications. I am going to present this problem under the speed dating scenario for easier understanding and use simulation to find the answer.&lt;/p&gt;
&lt;!--more--&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Imagine you are going to attend a speed dating event at Ritz Carlton on Friday night, You have paid $500 to the event organizer and are determined to find an idea partner in this luxury hotel. However, this event is not only planned and optimized for yourself, there are some rules on the board to help running the event smoothly. The headache guidelines include: 1.The Boy/Girl arrvies sequentially in random order and you have only 5 mins to talk with each one of them. 2. Once you decide to pass the current partner, he/she is gone forever and cannot be recalled. 3.Once you feel good enough about the current person, You could exchange the phone number with him/her and you have to leave the event immediately. (The rules, of course, are not entirely reasonable in real applications) Since you have spent lots of money and energy to join this event, you also set a high standard for yourself: to find the best Boy/Girl in this event, no one less will do. And you are experienced enough to order the person you met from most admired to worst with no ties in 5 mins. So the question comes: what kind of strategy should you adopt to find the best candidate ? At which point should you decide to stop and exchange your phone number with the opposite partner? In my simulation, I assume there are 100 candidates joining this event&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

set.seed(123)
best_k &amp;lt;- function(Simulations_N, N) {
    if (N == 0 || Simulations_N == 0) {
        return(&amp;quot;Number of Samples and Number of Simulations should be Greater Than Zero !&amp;quot;)
    }
    # Index represents the stopping place, the number is the time you select the
    # best candidate
    quality &amp;lt;- rep(0, N - 1)
    for (i in 1:Simulations_N) {
        # Randomly generate N numbers which indicate the quality of the candidates
        scores &amp;lt;- sample.int(N, N)
        for (j in 1:N - 1) {
            if (max(scores[1:j]) == N) {
                # Fail to find a good partner
            } else {
                # Find the first candidate that we see who is better than all of the
                # previous candidates
                optimal_place &amp;lt;- min(which(scores[j + 1:N] &amp;gt; max(scores[1:j]))) + 
                  j
                if (scores[optimal_place] == N) {
                  quality[j] = quality[j] + 1
                }
            }
        }
    }
    return(which.max(quality)/N)
}

# Increase the simulation N and to see the convergent result
simulations &amp;lt;- c(10, 50, 100, 200, 300, 400, 500, seq(1000, 30000, 1000))
result &amp;lt;- NULL

for (z in simulations) {
    result &amp;lt;- c(result, best_k(z, 100))
}

qplot(x = simulations, y = result, geom = c(&amp;quot;point&amp;quot;, &amp;quot;line&amp;quot;), xlab = &amp;quot;Sample Size&amp;quot;, 
    ylab = &amp;quot;Optimal Stopping&amp;quot;) + geom_hline(aes(yintercept = 0.37), color = &amp;quot;red&amp;quot;, 
    linetype = &amp;quot;dashed&amp;quot;) + geom_text(aes(0, 0.37, label = 0.37, vjust = -1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://songyang0716.github.io/post/2017-05-02-optimal-stopping-in-speed-dating_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whence-37&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Whence 37% ?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;This part is from the book “Algorithms to Live By” by Brian Christian and Tom Griffiths&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In your search for a lover, there are two ways you can fail: stopping earily, you leave the best choice undiscovered. When you stop too late, you hold out for a better choice who doesn’t exist. The optimal strategy will clearly require finding the right balance between the two, walking the tightrope between looking too much and not enough.&lt;/p&gt;
&lt;p&gt;Intuitively, there are a few potential strategies. For instance, making an offer the third time an applicant trumps eveyone seen so far- or maybe the fourth time. Or perhaps taking the next best-yet applicant to come along after a long drought. But as it happens, neither of these relatively sensible strategies comes out on top. Instead, the optimal solution takes the form of what we’ll call the &lt;strong&gt;Look-Then-Leap-Rule&lt;/strong&gt;. You set a predetermined amount of time for “looking” - that is, exploring your options, gathering data - in which you categorically don’t choose anyone, no matter how impressive. After that point, you enter the “leap” phase, prepared to instantly commit to anyone who outshines the best applicant you saw in the look phase.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Analysis of Premier League.rmd</title>
      <link>https://songyang0716.github.io/2017/04/an-analysis-of-premier-league.rmd/</link>
      <pubDate>Sat, 29 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://songyang0716.github.io/2017/04/an-analysis-of-premier-league.rmd/</guid>
      <description>&lt;p&gt;The &lt;strong&gt;Premier League&lt;/strong&gt; is an English professional league for men’s association football clubs. At the top of the English football league system, it is the country’s primary football competition. Contested by 20 clubs, it operates on a system of promotion and relegation with the English Football League. Premier League season runs from August to May, Teams play 38 matches each, totalling 380 matches in the season. I have to say that it’s my favorite league in the world, I do really enjoy the high competitiveness of Premier League, Also in the video game, it’s a wise choice to start your career mode using a Premier League team due to the high transfer budget.&lt;/p&gt;
&lt;!--more--&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;The soccer data is collected and prepared by James P. Curley (2016). engsoccerdata: English Soccer Data&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;import-and-clean-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Import and clean the data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats)
library(devtools)
# install_github(&amp;#39;jalapic/engsoccerdata&amp;#39;, username = &amp;#39;jalapic&amp;#39;)
library(engsoccerdata)
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(ggrepel)


df &amp;lt;- as_tibble(england)

# The dataset includes results of all top 4 tier soccer games in England
# since 1888, In our analysis, We are only interested in the Premier League
# which started in 1992.
df$date &amp;lt;- as.Date(df$Date, format = &amp;quot;%Y-%m-%d&amp;quot;)
pl &amp;lt;- df %&amp;gt;% filter(tier == 1, Season &amp;gt;= 1992)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visulizating-some-interesting-facts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visulizating some interesting facts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The followings are the 47 teams that ever played in Premier League for at least one season&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sort(unique(pl$home))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;AFC Bournemouth&amp;quot;         &amp;quot;Arsenal&amp;quot;                
##  [3] &amp;quot;Aston Villa&amp;quot;             &amp;quot;Barnsley&amp;quot;               
##  [5] &amp;quot;Birmingham City&amp;quot;         &amp;quot;Blackburn Rovers&amp;quot;       
##  [7] &amp;quot;Blackpool&amp;quot;               &amp;quot;Bolton Wanderers&amp;quot;       
##  [9] &amp;quot;Bradford City&amp;quot;           &amp;quot;Burnley&amp;quot;                
## [11] &amp;quot;Cardiff City&amp;quot;            &amp;quot;Charlton Athletic&amp;quot;      
## [13] &amp;quot;Chelsea&amp;quot;                 &amp;quot;Coventry City&amp;quot;          
## [15] &amp;quot;Crystal Palace&amp;quot;          &amp;quot;Derby County&amp;quot;           
## [17] &amp;quot;Everton&amp;quot;                 &amp;quot;Fulham&amp;quot;                 
## [19] &amp;quot;Hull City&amp;quot;               &amp;quot;Ipswich Town&amp;quot;           
## [21] &amp;quot;Leeds United&amp;quot;            &amp;quot;Leicester City&amp;quot;         
## [23] &amp;quot;Liverpool&amp;quot;               &amp;quot;Manchester City&amp;quot;        
## [25] &amp;quot;Manchester United&amp;quot;       &amp;quot;Middlesbrough&amp;quot;          
## [27] &amp;quot;Newcastle United&amp;quot;        &amp;quot;Norwich City&amp;quot;           
## [29] &amp;quot;Nottingham Forest&amp;quot;       &amp;quot;Oldham Athletic&amp;quot;        
## [31] &amp;quot;Portsmouth&amp;quot;              &amp;quot;Queens Park Rangers&amp;quot;    
## [33] &amp;quot;Reading&amp;quot;                 &amp;quot;Sheffield United&amp;quot;       
## [35] &amp;quot;Sheffield Wednesday&amp;quot;     &amp;quot;Southampton&amp;quot;            
## [37] &amp;quot;Stoke City&amp;quot;              &amp;quot;Sunderland&amp;quot;             
## [39] &amp;quot;Swansea City&amp;quot;            &amp;quot;Swindon Town&amp;quot;           
## [41] &amp;quot;Tottenham Hotspur&amp;quot;       &amp;quot;Watford&amp;quot;                
## [43] &amp;quot;West Bromwich Albion&amp;quot;    &amp;quot;West Ham United&amp;quot;        
## [45] &amp;quot;Wigan Athletic&amp;quot;          &amp;quot;Wimbledon&amp;quot;              
## [47] &amp;quot;Wolverhampton Wanderers&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;which clubs have never been relegated from the league ?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl_team &amp;lt;- pl %&amp;gt;%
       group_by(home) %&amp;gt;%
       summarize(appearence = n_distinct(Season))

pl_team %&amp;gt;%
  mutate(home = reorder(home, appearence)) %&amp;gt;%
  top_n(15) %&amp;gt;%
  ggplot(aes(x = reorder(home, -appearence), y = appearence)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, fill = &amp;#39;lightblue&amp;#39;, color = &amp;#39;black&amp;#39;) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  xlab(&amp;#39;Club&amp;#39;) +
  ylab(&amp;#39;Season&amp;#39;) +
  ggtitle(&amp;#39;Total Seasons in Premier League&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://songyang0716.github.io/post/premier_league_analysis_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since the start of the Premier League, seven clubs have never faced the drop: They are &lt;strong&gt;Arsenal, Aston Villa, Chelsea, Everton, Liverpool, Man Utd and Hotspur&lt;/strong&gt;. Hopefully, we could see Newcastle United in the next season ! &lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ranking of each club in the past 25 years&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl_rank &amp;lt;- pl %&amp;gt;%
           mutate(home_team_point = ifelse(hgoal &amp;gt; vgoal, 3, ifelse(hgoal == vgoal, 1, 0))) %&amp;gt;%
           mutate(away_team_point = ifelse(vgoal &amp;gt; hgoal, 3, ifelse(hgoal == vgoal, 1, 0)))
temp1 &amp;lt;-  pl_rank %&amp;gt;%
           group_by(Season, home) %&amp;gt;%
           summarize(home_points = sum(home_team_point))
temp2 &amp;lt;- pl_rank %&amp;gt;%
          group_by(Season, visitor) %&amp;gt;%
          summarize(away_points = sum(away_team_point))

temp1$away_points &amp;lt;- temp2$away_points
pl_rank &amp;lt;- temp1 %&amp;gt;%
           mutate(total_points = home_points + away_points)

pl_rank &amp;lt;- pl_rank %&amp;gt;%
           rename(Team = home) %&amp;gt;%
           group_by(Season) %&amp;gt;%
           arrange(-total_points) %&amp;gt;%
           mutate(ranking = row_number())

# We only include some famous football clubs in the visulization
pl_rank  %&amp;gt;%
    filter(Team %in% c(&amp;#39;Arsenal&amp;#39;,&amp;#39;Chelsea&amp;#39;,&amp;#39;Liverpool&amp;#39;,
                       &amp;#39;Manchester City&amp;#39;,&amp;#39;Manchester United&amp;#39;,&amp;#39;Tottenham Hotspur&amp;#39;)) %&amp;gt;%
    ggplot(aes(x = Season, y = ranking, group = Team, colour = Team)) +
    geom_line(size = 1) +
    geom_point() +
    scale_x_continuous(breaks=seq(1992,2015,1)) +
    theme(axis.text.x=element_text(angle=45, hjust=1)) +
    xlab(&amp;#39;Year&amp;#39;) +
    ylab(&amp;#39;Standing&amp;#39;) +
    ggtitle(&amp;#39;Top Team Standing&amp;#39;) +
    theme(legend.position=&amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://songyang0716.github.io/post/premier_league_analysis_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the plot, I notice Manchester United wins the most titles in the premier league history and fans are impressed by their stability and consistency before Ferguson’s retirement in 2012. On the other hand, Manchester City is not a championship competitor before, After being purchased by the &lt;strong&gt;Abu Dhabi United Group&lt;/strong&gt; in 2008, the club took transfer spending to an unprecendented level and the continued inverstment in players followed in gradually successive seasons.&lt;br /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;div id=&#34;could-defense-help-a-team-to-win-championships&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Could defense help a team to win championships ?&lt;/h3&gt;
&lt;p&gt;In 2004, Greece won the European Championship with a defense first approach. They did manage to score in the process, but it was their defense that allowed them to claim the title. In addition, &lt;strong&gt;Jose Mourinho&lt;/strong&gt; helped Inter Milan to win the 2010 Champions League by employing the so called ‘parking the bus’ strategy, avoiding attacking play and let all team chasing the ball in their half-court. Clearly, All of the examples I cited are in the tournament game. So I want to learn that if it’s possible for a team without explosive strikers but outstanding defensive still able to win the premier league ?&lt;/p&gt;
&lt;p&gt;I use the ranking of &lt;strong&gt;total goals a team has scored&lt;/strong&gt; to evaluate the ability of its offense, In the same way, I used the ranking of &lt;strong&gt;total goals a team have been scored&lt;/strong&gt; to evaluate the ability its defense&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl_team_overview &amp;lt;- pl %&amp;gt;%
           mutate(home_team_point = ifelse(hgoal &amp;gt; vgoal, 3, ifelse(hgoal == vgoal, 1, 0))) %&amp;gt;%
           mutate(away_team_point = ifelse(vgoal &amp;gt; hgoal, 3, ifelse(hgoal == vgoal, 1, 0)))

temp1 &amp;lt;-  pl_team_overview %&amp;gt;%
           group_by(Season, home) %&amp;gt;%
           summarize(home_points = sum(home_team_point), GF_Home = sum(hgoal), GA_Home = sum(vgoal))

temp2 &amp;lt;- pl_team_overview %&amp;gt;%
          group_by(Season, visitor) %&amp;gt;%
          summarize(away_points = sum(away_team_point), GF_Away = sum(vgoal), GA_Away = sum(hgoal))

temp1$away_points &amp;lt;- temp2$away_points
temp1$GF_Away &amp;lt;- temp2$GF_Away
temp1$GA_Away &amp;lt;- temp2$GA_Away

pl_team_overview &amp;lt;- temp1 %&amp;gt;%
           mutate(total_points = home_points + away_points) %&amp;gt;%
           mutate(GF = GF_Home + GF_Away) %&amp;gt;%
           mutate(GA = GA_Home + GA_Away) %&amp;gt;%
           mutate(goaldiff = GF - GA)

pl_team_overview &amp;lt;- pl_team_overview %&amp;gt;%
           rename(Team = home) %&amp;gt;%
           group_by(Season) %&amp;gt;%
           mutate(GF_ranking = rank(-GF, ties.method = &amp;quot;min&amp;quot;), GA_ranking = rank(GA, ties.method = &amp;quot;min&amp;quot;), points_ranking = order(order(-total_points, -goaldiff)))


# Plot the offense and defense Ranking of the Championship Team
pl_team_overview %&amp;gt;%
                  filter(points_ranking == 1) %&amp;gt;%
                  ggplot(aes(x = GF_ranking, y = GA_ranking)) +
                  geom_point(shape = 15, color = &amp;#39;deepskyblue3&amp;#39;, fill = &amp;#39;deepskyblue3&amp;#39;, size = 15, alpha = 1/3) +
                  geom_abline(intercept = 0, slope = 1, color=&amp;quot;red&amp;quot;, linetype=&amp;quot;dashed&amp;quot;) +
                  scale_x_continuous(name=&amp;quot;Goals Scored Ranking&amp;quot;, limits=c(1, 8), breaks = seq(1,8,1)) +
                  scale_y_continuous(name=&amp;quot;Goals Allowed Ranking&amp;quot;, limits=c(1, 8), breaks = seq(1,8,1)) +
                  ggtitle(&amp;#39;Premier League Championships offense and defense Ranking&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://songyang0716.github.io/post/premier_league_analysis_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the above plot, I noticed that most of the points cluster in the upper left part of the plot, which indicates all of past premier league championship teams at least have the top 3 offense, on the other hand, their defensive ability may not be prestigious (Top 7)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;could-defense-help-a-team-to-avoid-relegation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Could defense help a team to avoid relegation ?&lt;/h3&gt;
&lt;p&gt;Surviving Premier League relegation has always been lucrative, considering that teams staying afloat can retain around 100 million for the next season. Still, not eveyone can avoid the drop. So my question is that is it possible for a team with slightly above average defense but unproductive offense ensure survival ?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot the offense and defense Ranking of the Relegation Team
# In season 1992-1994, there are more than 20 teams in the league
relegation_team &amp;lt;- pl_team_overview %&amp;gt;%
                          filter((Season %in% (1995:2016) &amp;amp; points_ranking &amp;gt;= 18) |
                          (Season == 1992 &amp;amp; points_ranking &amp;gt;= 20) |
                          (Season == 1993 &amp;amp; points_ranking &amp;gt;= 20) |
                          (Season == 1994 &amp;amp; points_ranking &amp;gt;= 19)) %&amp;gt;%
                          select(Season, Team, GF_ranking, GA_ranking) %&amp;gt;%
                        rename(Offense = GF_ranking, Defense = GA_ranking)

# Convert the dataframe from wide to long using tidyr package
relegation_team_long &amp;lt;- gather(relegation_team, Offense_or_Defense, ranking, Offense, Defense)

relegation_team_long %&amp;gt;%
                ggplot(aes(x = ranking, fill = Offense_or_Defense)) +
                geom_density(alpha = 0.4) +
                scale_x_continuous(name=&amp;quot;Ranking&amp;quot;, limits=c(1, 22), breaks = seq(1,20,1)) +
                ggtitle(&amp;#39;Premier League Relegation Team&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://songyang0716.github.io/post/premier_league_analysis_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the above plot, Relegation teams always did worse in both offense and defense side. In addition, I notice that defense has higher density in the lower ranking side of the plot, which infers the relegation teams have lower average defense ranking than offense ranking. However, the difference is not significant. On the other hand, The basic lesson is that competence on both offense and defense with a slightly above average performance in one of the two areas (ranking in top 8) is enough to gurantee survival&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Review of Spelling Correction</title>
      <link>https://songyang0716.github.io/2017/04/a-review-of-spelling-correction/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://songyang0716.github.io/2017/04/a-review-of-spelling-correction/</guid>
      <description>&lt;p&gt;A tutorial on Non-Word Spelling Correction and Real Word Spelling Correction
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note : The ideas and contents of this tutorial is from Stanford Professor Dan Jurafsky&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;Recently, I am always interrupted by the annoying &lt;a href=&#34;https://www.grammarly.com/&#34;&gt;Grammarly&lt;/a&gt; ads on Youtube. In the same time, I can&amp;rsquo;t help to watch and astonished by the powerful features in this playform, one of the feature is to correct misspelled words in your email, flags words in a email that may not be spelled correctly. Then I decided to learn the ideas behind it. Spelling correction is often considered from two perspectives. &lt;strong&gt;Non-word spelling correction&lt;/strong&gt; is the detection and correction of spelling errors that result in non-words (like &lt;em&gt;speling&lt;/em&gt; for &lt;em&gt;spelling&lt;/em&gt;, &lt;em&gt;corection&lt;/em&gt; for &lt;em&gt;correction&lt;/em&gt;). By contrast, &lt;strong&gt;real word spelling correction&lt;/strong&gt; is the task of detecting and correcting spelling errors even if they accidentally result in an actual word of English (I like to eat Cheescake Factory &lt;em&gt;desert&lt;/em&gt;: which is suppose to be &lt;em&gt;dessert&lt;/em&gt;)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Non-word spelling errors&lt;/strong&gt; are detected by looking for any word not found in a dictionary. For example, the word &lt;em&gt;speling&lt;/em&gt; can&amp;rsquo;t be found in a dictionary. To correct non-word spelling errors we need to generate &lt;strong&gt;candidates&lt;/strong&gt; : real words that have a similar pattern to the error. Candidate corrections from the spelling error &lt;em&gt;speling&lt;/em&gt; might include &lt;em&gt;speeding&lt;/em&gt;, &lt;em&gt;spoiling&lt;/em&gt;, &lt;em&gt;spiking&lt;/em&gt;, &lt;em&gt;spending&lt;/em&gt; or &lt;em&gt;spelling&lt;/em&gt;. Then we rank the candidate using a &lt;strong&gt;distance metric&lt;/strong&gt; between the source and the surface error.  We want the word &lt;em&gt;spelling&lt;/em&gt; has the highest probability among all the candidates&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Real-word spelling error&lt;/strong&gt; detection is much more difficult, since any word in the input text could be an error. However, we could still find the candidate of all words that occurs in a sentence and rank all the combinations to find the users original intentional one&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;methods&#34;&gt;Methods&lt;/h3&gt;

&lt;p&gt;In order to learn the spelling correction, we need to introduce the &lt;strong&gt;noisy channel model&lt;/strong&gt; first and show how to apply it to the task of detecing and correcting spelling errors.&lt;/p&gt;

&lt;p&gt;The intuition of the &lt;strong&gt;noisy channel&lt;/strong&gt; model is to treat the misspelled word as if a correctly spelled word had been &amp;ldquo;&amp;ldquo;distorted&amp;rdquo; by being passed through a noisy communication channel. This channel introduces &amp;ldquo;noise&amp;rdquo; in the form of substitution or other changes to the letters, making it hard to recognize the &amp;ldquo;true&amp;rdquo; word. Our goal is to build a model of the channel. Given this model, we then find the true word by passing every candidate word of the language through our model of the noisy channel and seeing which one comes the closest to the misspelled word.&lt;/p&gt;

&lt;p&gt;The noisy channel is a kind of &lt;strong&gt;Bayesian Inference&lt;/strong&gt;. For each given misspelled word &lt;em&gt;x&lt;/em&gt;, Our job is to find one correct spelled word &lt;em&gt;w&lt;/em&gt; with the highest possibility.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                        w_hat = argmaxP(w|x)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;a-baby-version-of-word-spelling-corrector&#34;&gt;A baby version of Word Spelling Corrector&lt;/h3&gt;

&lt;h4 id=&#34;python-version&#34;&gt;Python Version&lt;/h4&gt;


  
    
  
  
    
  
  
    
  
  
    
  


&lt;figure class=&#34;highlight python&#34;&gt;
  &lt;figcaption&gt;
    
      &lt;span&gt;spell.py&lt;/span&gt;&lt;a href=&#34;http://underscorejs.org/#compact&#34; target=&#34;_blank&#34; rel=&#34;external&#34;&gt;spell.py&lt;/a&gt;
    
  &lt;/figcaption&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td class=&#34;gutter&#34;&gt;
          &lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;
        &lt;/td&gt;
        &lt;td class=&#34;code&#34;&gt;
          &lt;pre class=&#34;python code-highlight&#34;&gt;import re
from collections import Counter

def words(text) : return re.findall(r&amp;#39;\w&amp;#43;&amp;#39;, text.lower())

WORDS = Counter(words(open(&amp;#39;big.txt&amp;#39;).read()))

def P(word, N = sum(WORDS.values())):
    # Probability of the word
    return WORDS[word] * 1.0 / N

def correction(word):
    # Return the maximum prob word
    return max(candidate(word), key = P)

def candidate(word):
    return (known([word]) | known(edit1(word)) | known(edit2(word)) | {word})

def known(words):
    return set(w for w in words if w in WORDS)

def edit1(word):
    # All edit that are one edit away from word
    letters = &amp;#34;abcdefghijklmnopqrstuvwxyz&amp;#34;
    splits = [(word[:i], word[i:])for i in range(len(word) &amp;#43; 1)]
    deletes = {L &amp;#43; R[1:] for L, R in splits if R}
    inserts = {L &amp;#43; letter &amp;#43; R for L, R in splits for letter in letters}
    sub = {L &amp;#43; letter &amp;#43; R[1:] for L, R in splits if R for letter in letters}
    trans = {L &amp;#43; R[1] &amp;#43; R[0] &amp;#43; R[2:] for L, R in splits if len(R) &amp;gt; 1}
    return deletes | inserts | trans | sub | {word}

def edit2(word):
    return {e2 for e1 in edit1(word) for e2 in edit1(e1)}&lt;/pre&gt;
        &lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Hello World</title>
      <link>https://songyang0716.github.io/2017/04/hello-world/</link>
      <pubDate>Fri, 14 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://songyang0716.github.io/2017/04/hello-world/</guid>
      <description>&lt;p&gt;Welcome to my Den !
&lt;/p&gt;

&lt;h3 id=&#34;about-me&#34;&gt;About me&lt;/h3&gt;

&lt;p&gt;My name is Yang Song, I am a Data Scientist at &lt;a href=&#34;http://www.homedepot.com/&#34;&gt;The Home Depot&lt;/a&gt; - mainly focus on the &lt;a href=&#34;http://www.homedepot.com/services/&#34;&gt;Home Services Program&lt;/a&gt; and &lt;a href=&#34;https://www.proreferral.com/&#34;&gt;Pro Referral Platform&lt;/a&gt;. In June 2015 I received my Master&amp;rsquo;s Degree in Statistics from &lt;a href=&#34;http://www.stanford.edu/&#34;&gt;Stanford University&lt;/a&gt;. My interests include statistics, data analysis, data visulization, machine learning and programming in R and Python.&lt;/p&gt;

&lt;p&gt;I enjoy teaching to others for things that I understand, and also looking forward to learning from others. When I write code I usually do it in Python and R, but historically I spent some time in Java and C++. I also have fair amount of experiences using Tableau for visualization purpose.&lt;/p&gt;

&lt;p&gt;I am now living in the San Francisco Bay Area, I am a big sports fan, especially into Soccer and Basketball, &lt;a href=&#34;https://en.wikipedia.org/wiki/Beijing_Sinobo_Guoan_F.C.&#34;&gt;Guoan&lt;/a&gt; is my most favorite team and one of my childhood dreams is to become a part of it.&lt;/p&gt;

&lt;p&gt;Other random facts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I was grew up in Beijing and attended a &lt;a href=&#34;http://www.bjroyalschool.com/&#34;&gt;boarding school&lt;/a&gt; since 6 years old&lt;/li&gt;
&lt;li&gt;I started playing &lt;a href=&#34;https://en.wikipedia.org/wiki/FIFA_(video_game_series)&#34;&gt;FIFA soccer&lt;/a&gt; since 1999 and that&amp;rsquo;s my only favorite game&lt;/li&gt;
&lt;li&gt;I have more than 10 pairs of &lt;a href=&#34;https://en.wikipedia.org/wiki/Air_Jordan&#34;&gt;Air Jordan Shoes&lt;/a&gt;, the first pair is the AJ XII&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;about-this-blog&#34;&gt;About this blog&lt;/h3&gt;

&lt;p&gt;I have been inspired by the active community of data science and benefited significantly from reading and following others&amp;rsquo; blogs and tutorials. My favorite data science blogs include, but not limited to &lt;a href=&#34;https://yihui.name/en/about/&#34;&gt;Yihui Xie&lt;/a&gt;, &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;David Robinson&lt;/a&gt;, &lt;a href=&#34;http://juliasilge.com/&#34;&gt;Julia Silge&lt;/a&gt; and &lt;a href=&#34;https://sebastianraschka.com/blog/index.html&#34;&gt;Sebastian Raschka&lt;/a&gt;. After learning from these selfless people &amp;amp; teams, I decided to create my own portfolio to keep track on my learning process as well as share my knowledge to others. This blog will focus on natural language processing, data analysis, machine learning and data visualization by using real-world dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I would like to hear from you if there is any question or feedback, My email address is yangsong0716@gmail.com&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This blog is created using the new R blogdown package which integrates Github and Hugo. To learn more about it, I recommend this great &lt;a href=&#34;https://tclavelle.github.io/blog/blogdown_github/&#34;&gt;tutorial&lt;/a&gt; by Tyler Clavelle&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>